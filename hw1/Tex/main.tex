\documentclass[12pt,letterpaper]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{enumitem}

\input{macros.tex}

% info for header block in upper right hand corner
\name{Eli Weissler}
\class{Math189R SP19}
\assignment{Homework 1}
\duedate{Monday, February 03, 2019}

\renewcommand{\labelenumi}{{(\alph{enumi})}}


\begin{document}
Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
The starter code for problem 2 part c and d can be found under the Resource tab on course website.\\

\textit{Note:} You need to create a Github account for submission of the coding part of the homework. Please create a repository on Github to hold all your code and include your Github account username as part of the answer to problem 2.

\begin{problem}[1]
(\textbf{Linear Transformation}) Let $\mathbf{y} = A\mathbf{x} + \mathbf{b}$ be a random vector.
show that expectation is linear:
\[
    \EE[\yy] = \EE[A\xx + \bb] = A\EE[\xx] + \bb.
\]
Also show that
\[
    \cov[\yy] = \cov[A\xx + \bb] = A \cov[\xx] A^\T = A\Sigmab A^\T.
\]
\end{problem}
\begin{solution}
First recall the definition of expectation value $\EE$ of a vector $\xx$
\begin{align*}
    \EE[\xx] & = \begin{pmatrix}\EE[x_1] \\ \EE[x_2] \\ \cdots \\ \EE[x_i]\\ \cdots \end{pmatrix} = \begin{pmatrix}\int x_1 f(x_1)dx_1 \\ \int x_2 f(x_2)dx_2 \\ \cdots \\ \int x_i f(x_i)dx_i\\ \cdots \end{pmatrix}
\end{align*}
where $f(x_i)$ is some probability density function, and the integral is performed over the kernel. In this case, we assume that $\xx$ is a random vector with $n$ continuous random variables, $A$ is a constant matrix, and $\bb$ is a constant vector.
\begin{align*}
    \EE[A\xx + \bb] &= \begin{pmatrix}\EE[A_{1:}\cdot\xx + b_1] \\ \EE[A_{2:}\cdot\xx + b_2] \\ \cdots \\ \EE[A_{i:}\cdot\xx + b_i]\\ \cdots \end{pmatrix}
\end{align*}
Note that I am using $A_{i:}$ to denote the row vector formed by all elements in row $i$ of $A$. If we examine an arbitrary element, we see that
\begin{align*}
    \EE[A_{i:}\cdot\xx + b_i] = \EE[\sum_j A_{ij}x_k + b_i] \\
\end{align*}
Now we use the formal definition of expectation value to convert to an integral. Although we have to integrate over all $x_j$, unless a term contains an $x_j$ the integral goes to one.
\begin{align*}
    \EE[\sum_j A_{ij}x_k + b_i] &= \int\int...\int f(x_1)dx_1f(x_2)dx_2...f(x_n)dx_n(\sum_j A_{ij} x_j f(x_j) + b_i) \\
    &= \sum_j \int A_{ij}x_jf(x_j) dx_j + b_i = \sum_j A_{ij}\int x_jf(x_j) dx_j + b_i \\
    &= \sum_j A_{ij} \EE[x_j] + b_i = A_{i:}\cdot \EE[\xx] + b_i
\end{align*}
Bringing this back together, we see that
\begin{align*}
    \EE[A\xx + \bb] &= \begin{pmatrix}A_{1:}\cdot \EE[\xx] + b_1 \\ A_{2:}\cdot \EE[\xx] + b_2 \\ \cdots \\ A_{i:}\cdot \EE[\xx] + b_i\\ \cdots \end{pmatrix} = A\EE[\xx] + \bb
\end{align*}
as desired. \newline \break 
Now recall the definition of covariance
\begin{align*}
    \textrm{cov}[\xx]_{ij} = \EE[(x_i - \EE[x_i])(x_j - \EE[x_j])]
\end{align*}
If we make the same assumptions, i.e. that $\xx$ is a random vector with $n$ continous variables, $A$ is a constant matrix, and $\bb$ is a constant vector, then
\begin{align*}
    cov[A\xx + \bb]_{ij} &= \EE[((A\xx + \bb)_i - \EE[(A\xx + \bb)_i])((A\xx + \bb)_j - \EE[(A\xx + \bb)_j])] \\
    &= \EE[(A_{i:}\cdot \xx + b_i - A_{i:}\cdot \EE[x] - b_i)(A_{j:}\cdot \xx + b_j - A_{j:}\cdot \EE[x] - b_j)] \\
    &= \EE[(A_{i:}\cdot \xx - A_{i:}\cdot \EE[x] - )(A_{j:}\cdot \xx  - A_{j:}\cdot \EE[x] )] \\
    &= \EE[(A_{i:}\cdot \xx - A_{i:}\cdot \EE[x] - )(\xx^T \cdot A^T_{:j} -  \EE[x]^T \cdot A^T_{:j} )] \quad \textrm{Rewriting the right parenthesis} \\
    &= \EE[A_{i:}\cdot(\xx - \EE[\xx])(\xx^T - \EE[\xx]^T)\cdot A_{:j}]\\
    &= A_{i:}\cdot\EE[(\xx - \EE[\xx])(\xx^T - \EE[\xx]^T)]\cdot A_{:j} \\
    &= (A \textrm{cov}[\xx]A^T)_{ij}
\end{align*}
Therefore $\cov[A\xx + \bb] = A \cov[\xx] A^\T$ as desired.
\vfill

\end{solution}
\newpage




\begin{problem}[2]
Given the dataset $\Dc = \{(x,y)\} = \{(0,1), (2,3), (3,6), (4,8)\}$
\begin{enumerate}
   \item Find the least squares estimate $y = \thetab^\T\xx$ by hand using
        Cramer's Rule.
    \item Use the normal equations to find the same solution and verify it
        is the same as part (a).
    \item Plot the data and the optimal linear fit you found.
    \item Find randomly generate 100 points near the line with white Gaussian
        noise and then compute the least squares estimate (using a computer).
        Verify that this new line is close to the original and plot the new
        dataset, the old line, and the new line.
\end{enumerate}

\end{problem}
\begin{solution}
\begin{enumerate}
   \item 
   We are trying to minimize the cost function
   $$
   J = \sum[y_i - (mx_i + b)]^2
   $$
   We can do this by setting the partials with respect to $m,b$ equal to zero.
   \begin{align*}
       &\frac{\partial J}{\partial m} = \sum 2[y_i - (mx_i + b)](-x_i) = 0 \\
       &\frac{\partial J}{\partial b} = \sum 2[y_i - (mx_i + b)](-1) = 0 \\
       & \rightarrow \sum x_i y_i = m\sum x_i^2 + b\sum x_i\\
       & \rightarrow \sum y_i = m\sum x_i + bN
   \end{align*}
   In this problem, this is equivalent to the matrix
   \begin{align*}
       \begin{pmatrix} \sum x_i y_i \\ \sum y_i\end{pmatrix} &= \begin{pmatrix} \sum x_i^2 & \sum x_i \\ \sum x_i & N \end{pmatrix} \begin{pmatrix} m \\ b\end{pmatrix} \\
       \begin{pmatrix} 56 \\ 18\end{pmatrix} &= \begin{pmatrix} 29 & 9 \\ 9 & 4 \end{pmatrix} \begin{pmatrix} m \\ b\end{pmatrix}
   \end{align*}
   Using Cramer's rule from class, we get
   \begin{align*}
       &m = \frac{62}{35} \\
       &b = \frac{18}{35}
   \end{align*}
    \item 
    Define our feature matrix and outcomes to be
    \begin{align*}
        X = \begin{pmatrix} 1 & 0 \\ 1 & 2 \\1 & 3 \\ 1 & 4 \end{pmatrix} \qquad Y = \begin{pmatrix} 1 \\ 3 \\ 6 \\ 8 \end{pmatrix}
    \end{align*}
   Which yields (I multiplied matrices using Mathematica)
    \begin{align*}
        \theta = (X^TX)^{-1}(X^{T}Y) = \begin{pmatrix} \frac{18}{35} \\ \frac{62}{35} \end{pmatrix}
    \end{align*}
    Which agrees with the results from part (a), as $\theta_0 = b$ and $\theta_1 = m$.
    \item
    \includegraphics[]{hw1pr2c.png}
    \item 
    \includegraphics[]{hw1pr2d.png}
\end{enumerate}
    \vfill
\end{solution}
\newpage



\end{document}

