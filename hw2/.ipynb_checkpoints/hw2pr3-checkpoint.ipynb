{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start file for hw2pr3 of Big Data Summer 2017\n",
    "\n",
    "The file is seperated into two parts:\n",
    "\t1) the helper functions\n",
    "\t2) the main driver.\n",
    "\n",
    "The helper functions are all functions necessary to finish the problem.\n",
    "The main driver will use the helper functions you finished to report and print\n",
    "out the results you need for the problem.\n",
    "\n",
    "Before attemping the helper functions, please familiarize with pandas and numpy\n",
    "libraries. Tutorials can be found online:\n",
    "http://pandas.pydata.org/pandas-docs/stable/tutorials.html\n",
    "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\n",
    "\n",
    "First, fill in the the code of step 0 in the main driver to load the data, then\n",
    "please COMMENT OUT any steps in main driver before you finish the corresponding\n",
    "functions for that step. Otherwise, you won't be able to run the program\n",
    "because of errors.\n",
    "\n",
    "After finishing the helper functions for each step, you can uncomment\n",
    "the code in main driver to check the result.\n",
    "\n",
    "Note:\n",
    "1. When filling out the functions below, remember to\n",
    "\t1) Let m be the number of samples\n",
    "\t2) Let n be the number of features\n",
    "\n",
    "2. Please read the instructions and hints carefully, and use the name of the\n",
    "variables we provided, otherwise, the function may not work.\n",
    "\n",
    "3. Remember to comment out the TODO comment after you finish each part.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data...\n",
      "==> Successfully Loaded data...\n",
      "==> Step 1: RMSE vs lambda...\n",
      "==> Plotting completed.\n",
      "==> The optimal regularization parameter is  8.6431.\n",
      "==> The RMSE on the validation set with the optimal regularization parameter is  0.8340.\n",
      "==> The RMSE on the test set with the optimal regularization parameter is  0.8628.\n",
      "\n",
      "==> Step 2: Norm vs lambda...\n",
      "==> Plotting completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################\n",
    "#\t    \tMain Driver Function       \t  #\n",
    "###########################################\n",
    "\n",
    "# =============STEP 0: LOADING DATA=================\n",
    "print('==> Loading data...')\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('https://math189sp19.github.io/data/online_news_popularity.csv', \\\n",
    "    sep=', ', engine='python')\n",
    "\n",
    "print('==> Successfully Loaded data...')\n",
    "\n",
    "# split the data frame by type: training, validation, and test\n",
    "train_pct = 2.0 / 3\n",
    "val_pct = 5.0 / 6\n",
    "\n",
    "df['type'] = ''\n",
    "df.loc[:int(train_pct * len(df)), 'type'] = 'train'\n",
    "df.loc[int(train_pct * len(df)) : int(val_pct * len(df)), 'type'] = 'val'\n",
    "df.loc[int(val_pct * len(df)):, 'type'] = 'test'\n",
    "\n",
    "\n",
    "# extracting columns into training, validation, and test data\n",
    "X_train = np.array(df[df.type == 'train'][[col for col in df.columns \\\n",
    "    if col not in ['url', 'shares', 'type']]])\n",
    "y_train = np.array(np.log(df[df.type == 'train'].shares)).reshape((-1, 1))\n",
    "\n",
    "X_val = np.array(df[df.type == 'val'][[col for col in df.columns \\\n",
    "    if col not in ['url', 'shares', 'type']]])\n",
    "y_val = np.array(np.log(df[df.type == 'val'].shares)).reshape((-1, 1))\n",
    "\n",
    "X_test = np.array(df[df.type == 'test'][[col for col in df.columns \\\n",
    "    if col not in ['url', 'shares', 'type']]])\n",
    "y_test = np.array(np.log(df[df.type == 'test'].shares)).reshape((-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# HINT:\n",
    "# \t1) Use np.ones / np.ones_like to create a column of ones\n",
    "#\t2) Use np.hstack to stack the column to the matrix\n",
    "\"*** YOUR CODE HERE ***\"\n",
    "\n",
    "X_train = np.hstack((np.ones((X_train.shape[0],1)),X_train))\n",
    "X_val = np.hstack((np.ones((X_val.shape[0],1)),X_val))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test))\n",
    "\n",
    "\"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "# Convert data to matrix\n",
    "X_train = np.matrix(X_train)\n",
    "y_train = np.matrix(y_train)\n",
    "X_val = np.matrix(X_val)\n",
    "y_val = np.matrix(y_val)\n",
    "X_test = np.matrix(X_test)\n",
    "y_test = np.matrix(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# PART C\n",
    "# =============STEP 1: RMSE vs lambda=================\n",
    "# NOTE: Fill in code in linreg, findRMSE, and RMSE_vs_lambda for this step\n",
    "\n",
    "print('==> Step 1: RMSE vs lambda...')\n",
    "\n",
    "# find the optimal regularization parameter\n",
    "reg_opt = RMSE_vs_lambda(X_train, y_train, X_val, y_val)\n",
    "print('==> The optimal regularization parameter is {reg: 4.4f}.'.format(\\\n",
    "    reg=reg_opt))\n",
    "\n",
    "# Find the optimal weights and bias for future use in step 3\n",
    "W_with_b_1 = linreg(X_train, y_train, reg=reg_opt)\n",
    "b_opt_1 = W_with_b_1[0]\n",
    "W_opt_1 = W_with_b_1[1: ]\n",
    "\n",
    "# Report the RMSE with the found optimal weights on validation set\n",
    "val_RMSE = find_RMSE(W_with_b_1, X_val, y_val)\n",
    "print('==> The RMSE on the validation set with the optimal regularization parameter is {RMSE: 4.4f}.'.format(\\\n",
    "    RMSE=val_RMSE))\n",
    "\n",
    "# Report the RMSE with the found optimal weights on test set\n",
    "test_RMSE = find_RMSE(W_with_b_1, X_test, y_test)\n",
    "print('==> The RMSE on the test set with the optimal regularization parameter is {RMSE: 4.4f}.'.format(\\\n",
    "    RMSE=test_RMSE))\n",
    "\n",
    "\n",
    "\n",
    "# =============STEP 2: Norm vs lambda=================\n",
    "# NOTE: Fill in code in norm_vs_lambda for this step\n",
    "\n",
    "print('\\n==> Step 2: Norm vs lambda...')\n",
    "norm_vs_lambda(X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linreg(X, y, reg=0.0):\n",
    "    \"\"\"\tThis function takes in three arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) y, the label of the data with dimension m x 1\n",
    "            3) reg, the parameter for regularization\n",
    "\n",
    "        This function calculates and returns the optimal weight matrix, W_opt.\n",
    "\n",
    "        HINT: Find the numerical solution for part C\n",
    "            1) use np.eye to create identity matrix\n",
    "            2) use np.linalg.solve to solve for W_opt\n",
    "    \"\"\"\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "\n",
    "    n = X.shape[1] - 1\n",
    "\n",
    "    #Use the formula from part c. Don't have a penalty for the constant term\n",
    "    dontReg = reg*np.eye(n+1)\n",
    "    dontReg[0] = 0\n",
    "    W_opt = np.linalg.solve(np.dot(np.transpose(X),X) + dontReg,np.dot(np.transpose(X),y))\n",
    "\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return W_opt\n",
    "\n",
    "def predict(W, X):\n",
    "    \"\"\"\tThis function takes in two arguments:\n",
    "            1) W, a weight matrix with bias\n",
    "            2) X, the data with dimension m x (n + 1)\n",
    "\n",
    "        This function calculates and returns the predicted label, y_pred.\n",
    "\n",
    "        NOTE: You don't need to change this function.\n",
    "    \"\"\"\n",
    "    return X * W\n",
    "\n",
    "\n",
    "def find_RMSE(W, X, y):\n",
    "    \"\"\"\tThis function takes in three arguments:\n",
    "            1) W, a weight matrix with bias\n",
    "            2) X, the data with dimension m x (n + 1)\n",
    "            3) y, the label of the data with dimension m x 1\n",
    "\n",
    "        This function calculates and returns the root mean-squared error, RMSE\n",
    "    \"\"\"\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    \n",
    "    #Get prediction and subtract from actual\n",
    "    err = y - predict(W,X)\n",
    "    \n",
    "    #Now square everything\n",
    "    RMSE = np.linalg.norm(err)/np.sqrt(X.shape[0])\n",
    "    \n",
    "    \n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return RMSE\n",
    "\n",
    "\n",
    "\n",
    "def RMSE_vs_lambda(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\tThis function takes in four arguments:\n",
    "            1) X_train, the training data with dimension m x (n + 1)\n",
    "            2) y_train, the label of training data with dimension m x 1\n",
    "            3) X_val, the validation data with dimension m x (n + 1)\n",
    "            4) y_val, the label of validation data with dimension m x 1\n",
    "\n",
    "        This function generates a plot of RMSE vs lambda and returns the\n",
    "        regularization parameter that minimizes RMSE, reg_opt.\n",
    "\n",
    "        HINT: get a list of RMSE following the steps below:\n",
    "            1) Constuct reg_list, a list of regularization parameters with\n",
    "               random uniform sampling\n",
    "            2) Generate W_list, a list of W_opt's according to regularization\n",
    "               parameters generated above\n",
    "            3) Generate, RMSE_list, a list of RMSE according to reg_list\n",
    "    \"\"\"\n",
    "    RMSE_list = []\n",
    "    reg_list = [0]*1000\n",
    "    W_list = []\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    reg_list = np.sort([np.random.uniform(0.0,150.0) for x in reg_list])\n",
    "    for reg in reg_list:\n",
    "        W = linreg(X_train, y_train,reg)\n",
    "        RMSE = find_RMSE(W,X_val,y_val)\n",
    "        W_list = W_list + [W]\n",
    "        RMSE_list = RMSE_list + [RMSE]\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "    # Set up plot style\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    # Plot RMSE vs lambda\n",
    "    RMSE_vs_lambda_plot, = plt.plot(reg_list, RMSE_list)\n",
    "    plt.setp(RMSE_vs_lambda_plot, color='red')\n",
    "    plt.title('RMSE vs lambda')\n",
    "    plt.xlabel('lambda')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.savefig('RMSE_vs_lambda.png', format='png')\n",
    "    plt.close()\n",
    "    print('==> Plotting completed.')\n",
    "\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    best = np.argmin(RMSE_list)\n",
    "    reg_opt = reg_list[best]\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return reg_opt\n",
    "\n",
    "def norm_vs_lambda(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\tThis function takes in four arguments:\n",
    "            1) X_train, the training data with dimension m x (n + 1)\n",
    "            2) y_train, the label of training data with dimension m x 1\n",
    "            3) X_val, the validation data with dimension m x (n + 1)\n",
    "            4) y_val, the label of validation data with dimension m x 1\n",
    "\n",
    "        This function generates a plot of norm of the weights vs lambda.\n",
    "\n",
    "        HINT:\n",
    "            1) You may reuse the code from RMSE_vs_lambda to generate\n",
    "               w_list, the list of weights, and reg_list, the list of\n",
    "               regularization parameters\n",
    "            2) Then generate norm_list, a list of norm by calculating the\n",
    "               norm of each weight\n",
    "    \"\"\"\n",
    "    reg_list = [0]*1000\n",
    "    W_list = []\n",
    "    norm_list = []\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    reg_list = np.sort([np.random.uniform(0.0,150.0) for x in reg_list])\n",
    "    for reg in reg_list:\n",
    "        W = linreg(X_train, y_train,reg)\n",
    "        norm = np.linalg.norm(W)\n",
    "        W_list = W_list + [W]\n",
    "        norm_list = norm_list + [norm]\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "    # Set up plot style\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    # Plot norm vs lambda\n",
    "    norm_vs_lambda_plot, = plt.plot(reg_list, norm_list)\n",
    "    plt.setp(norm_vs_lambda_plot, color='blue')\n",
    "    plt.title('norm vs lambda')\n",
    "    plt.xlabel('lambda')\n",
    "    plt.ylabel('norm')\n",
    "    plt.savefig('norm_vs_lambda.png', format='png')\n",
    "    plt.close()\n",
    "    print('==> Plotting completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Step 3: Linear regression without bias...\n",
      "--Time elapsed for training: 24.66 seconds\n",
      "==> Difference in bias is  4.3336E-10\n",
      "==> Difference in weights is  5.7137E-10\n"
     ]
    }
   ],
   "source": [
    "# PART D\n",
    "# =============STEP 3: Linear regression without bias=================\n",
    "# NOTE: Fill in code in linreg_no_bias for this step\n",
    "\n",
    "# From here on, we will strip the columns of ones for all data\n",
    "X_train_noB = X_train[:, 1:]\n",
    "X_val_noB = X_val[:, 1:]\n",
    "X_test_noB = X_test[:, 1:]\n",
    "\n",
    "# Compare the result with the one from step 1\n",
    "# The difference in norm should be a small scalar (i.e, 1e-10)\n",
    "print('\\n==> Step 3: Linear regression without bias...')\n",
    "b_opt_2, W_opt_2 = linreg_no_bias(X_train_noB, y_train, reg=reg_opt)\n",
    "\n",
    "# difference in bias\n",
    "diff_bias = np.linalg.norm(b_opt_2 - b_opt_1)\n",
    "print('==> Difference in bias is {diff: 4.4E}'.format(diff=diff_bias))\n",
    "\n",
    "# difference in weights\n",
    "diff_W = np.linalg.norm(W_opt_2 -W_opt_1)\n",
    "print('==> Difference in weights is {diff: 4.4E}'.format(diff=diff_W))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_no_bias(X, y, reg=0.0):\n",
    "    \"\"\"\tThis function takes in three arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) y, the label of the data with dimension m x 1\n",
    "            3) reg, the parameter for regularization\n",
    "\n",
    "        This function calculates and returns the optimal weight matrix, W_opt\n",
    "        and bias, b_opt seperately\n",
    "    \"\"\"\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Find the numerical solution in part d\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    one = np.ones((m,1))\n",
    "    \n",
    "    #note that n in the answer to part c is actually the number of data points, so n here\n",
    "    W_opt = np.linalg.solve(X.T@(np.eye(m) - (1/m)*one@one.T) @ X + reg*np.eye(n),\n",
    "                               X.T@(np.eye(m) - (1/m)*one@one.T)@y)\n",
    "    \n",
    "    b_opt = one.T @ (y - X @ W_opt)/m\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "    # Benchmark report\n",
    "    t_end = time.time()\n",
    "    print('--Time elapsed for training: {t:4.2f} seconds'.format(\\\n",
    "            t=t_end - t_start))\n",
    "\n",
    "    return b_opt, W_opt\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Step 4: Gradient descent\n",
      "==> Running gradient descent...\n",
      "-- Iteration250 - training rmse  2.4167 - gradient norm  1.7517E+09\n",
      "-- Iteration500 - training rmse  2.1049 - gradient norm  2.4179E+09\n",
      "-- Iteration750 - training rmse  1.6437 - gradient norm  3.7421E+09\n",
      "-- Iteration1000 - training rmse  1.0786 - gradient norm  5.9191E+09\n",
      "--Time elapsed for training: 62.21 seconds\n",
      "==> Plotting completed.\n",
      "==> Difference in bias is  3.9596E-01\n",
      "==> Difference in weights is  7.9972E-01\n"
     ]
    }
   ],
   "source": [
    "# PART E\n",
    "# =============STEP 4: Gradient descent=================\n",
    "# NOTE: Fill in code in grad_descent for this step\n",
    "\n",
    "print('\\n==> Step 4: Gradient descent')\n",
    "b_gd, W_gd = grad_descent(X_train_noB, y_train, X_val_noB, y_val, W_opt_1,b_opt_1,reg=reg_opt)\n",
    "\n",
    "# Compare the result from the one from step 1\n",
    "# Difference in bias\n",
    "diff_bias = np.linalg.norm(b_gd - b_opt_1)\n",
    "print('==> Difference in bias is {diff: 4.4E}'.format(diff=diff_bias))\n",
    "\n",
    "# difference in weights\n",
    "diff_W = np.linalg.norm(W_gd -W_opt_1)\n",
    "print('==> Difference in weights is {diff: 4.4E}'.format(diff=diff_W))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(X_train, y_train, X_val, y_val, W_opt, b_opt,reg=0.0, lr_W=2.5e-12*1e-6, \\\n",
    "    lr_b=0.2*2e-7, max_iter=1100, eps=1e-6, print_freq=250):\n",
    "    \"\"\"\tThis function takes in ten arguments:\n",
    "        1) X_train, the training data with dimension m x (n + 1)\n",
    "        2) y_train, the label of training data with dimension m x 1\n",
    "        3) X_val, the validation data with dimension m x (n + 1)\n",
    "        4) y_val, the label of validation data with dimension m x 1\n",
    "        5) reg, the parameter for regularization\n",
    "        6) lr_W, the learning rate for weights\n",
    "        7) lr_b, the learning rate for bias\n",
    "        8) max_iter, the maximum number of iterations\n",
    "        9) eps, the threshold of the norm for the gradients\n",
    "        10) print_freq, the frequency of printing the report\n",
    "\n",
    "    This function returns W, the optimal weight, and b, the bias by\n",
    "    gradient descent.\n",
    "    \"\"\"\n",
    "    m_train, n = X_train.shape\n",
    "    m_val = X_val.shape[0]\n",
    "\n",
    "\n",
    "    # Please use the variable names: W (weights), W_grad (gradients of W),\n",
    "    # b (bias), b_grad (gradients of b)\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    #Get data dimensions. Sets have same # of cols, but diff # of rows\n",
    "    m_train = X_train.shape[0]\n",
    "    m_val = X_val.shape[0]\n",
    "    one_train = np.ones((m_train,1))\n",
    "    one_val = np.ones((m_val,1))\n",
    "    \n",
    "    n = X_train.shape[1]\n",
    "    \n",
    "    #Initialize weights and biases. Start weights at 0 and grad at 1\n",
    "    W = np.zeros((n,1))\n",
    "    b = 0\n",
    "    #W_grad = gradW(X_train,y_train,b,W,one_train,reg)\n",
    "    #b_grad = gradB(X_train,y_train,b,W,one_train,n)\n",
    "    W_grad = np.ones((n,1))\n",
    "    b_grad = 1\n",
    "\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "\n",
    "    print('==> Running gradient descent...')\n",
    "\n",
    "    # HINT: Run the gradient descent algorithm followed steps below\n",
    "    #\t1) Calculate the training RMSE and validation RMSE at each iteration,\n",
    "    #      and append these values to obj_train and obj_val respectively\n",
    "    #\t2) Calculate the gradient for W and b as W_grad and b_grad\n",
    "    #\t3) Upgrade W and b\n",
    "    #\t4) Keep iterating while the number of iterations is less than the\n",
    "    #\t   maximum and the gradient is larger than the threshold\n",
    "\n",
    "    obj_train = []\n",
    "    obj_val = []\n",
    "    norms_w = []\n",
    "    norms_b = []\n",
    "    iter_num = 0\n",
    "    \n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    # start iteration for gradient descent\n",
    "    while np.linalg.norm(W_grad) > eps and np.linalg.norm(b_grad) > eps \\\n",
    "        and iter_num < max_iter:\n",
    "\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        #Get RMSE's\n",
    "        \n",
    "        #Get prediction and subtract from actual\n",
    "        err_train = y_train - predict(W,X_train) - b\n",
    "        err_val = y_val - predict(W,X_val) - b\n",
    "\n",
    "        #Now get rmse\n",
    "        RMSE_train = np.linalg.norm(err_train)/np.sqrt(m_train)\n",
    "        RMSE_val = np.linalg.norm(err_val)/np.sqrt(m_val)\n",
    "        \n",
    "        obj_train = obj_train + [RMSE_train]\n",
    "        obj_val = obj_val + [RMSE_val]\n",
    "        norms_w = norms_w + [np.linalg.norm(W - W_opt)]\n",
    "        norms_b = norms_b + [np.absolute(b-b_opt)]\n",
    "        \n",
    "        \n",
    "        #Gradients from part d\n",
    "        W_grad = 2*(X_train.T @ X_train @ W) + 2*(b * X_train.T @ one_train) - 2*(X_train.T @ y_train) + 2*reg*W\n",
    "        b_grad = 2*(one_train.T @ X_train @ W) - 2*(one_train.T @ y_train) + 2*b*n\n",
    "        \n",
    "        #My gradient seems to be off from what you wrote by a factor of 1/m_train, but I don't see where\n",
    "        #that comes from in the math. Mine still converges to the right answer, albeit not as fast.\n",
    "        \n",
    "#         W_grad = ((X_train.T @ X_train + reg * np.eye(n)) @ W \\\n",
    "#             + X_train.T @ (b - y_train)) / m_train\n",
    "#         b_grad = (sum(X_train @ W) - sum(y_train) + b * m_train) / m_train\n",
    "\n",
    "        #now move along the negative gradient by learning_rate*gradient\n",
    "        W = W - W_grad*lr_W\n",
    "        b = b - float(b_grad*lr_b)\n",
    "\n",
    "        \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "        # print statements for debugging\n",
    "        if (iter_num + 1) % print_freq == 0:\n",
    "            print('-- Iteration{} - training rmse {: 4.4f} - gradient norm {: 4.4E}'.format(\\\n",
    "                iter_num + 1, RMSE_train, np.linalg.norm(W_grad)))\n",
    "\n",
    "        # goes to next iteration\n",
    "        iter_num += 1\n",
    "\n",
    "\n",
    "    # Benchmark report\n",
    "    t_end = time.time()\n",
    "    print('--Time elapsed for training: {t:4.2f} seconds'.format(\\\n",
    "        t=t_end - t_start))\n",
    "\n",
    "    # Set up plot style\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    # generate convergence plot\n",
    "    train_rmse_plot, = plt.plot(range(iter_num), obj_train)\n",
    "    plt.setp(train_rmse_plot, color='red')\n",
    "    val_rmse_plot, = plt.plot(range(iter_num), obj_val)\n",
    "    plt.setp(val_rmse_plot, color='green')\n",
    "    plt.legend((train_rmse_plot, val_rmse_plot), \\\n",
    "    ('Training RMSE', 'Validation RMSE'), loc='best')\n",
    "    plt.title('RMSE vs iteration')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.savefig('convergence.png', format='png')\n",
    "    plt.close()\n",
    "    \n",
    "    #generate norm plot\n",
    "    train_rmse_plot, = plt.plot(range(iter_num), np.ndarray.flatten(np.array(norms_w)))\n",
    "    plt.setp(train_rmse_plot, color='red')\n",
    "    val_rmse_plot, = plt.plot(range(iter_num), np.ndarray.flatten(np.array(norms_b)))\n",
    "    plt.setp(val_rmse_plot, color='green')\n",
    "    plt.legend((train_rmse_plot, val_rmse_plot), \\\n",
    "    ('W norm', 'b norm'), loc='best')\n",
    "    plt.title('norms vs iteration')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('W norm')\n",
    "    plt.savefig('W_convergence.png', format='png')\n",
    "    plt.close()\n",
    "    print('==> Plotting completed.')\n",
    "\n",
    "    return b, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
