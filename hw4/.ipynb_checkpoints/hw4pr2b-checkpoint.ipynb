{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Loading data...\n",
      "==>Data loaded succesfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Eli/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/Eli/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import p2_data as data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "\n",
    "# =============STEP 0: LOADING DATA=================\n",
    "# NOTE: The data is loaded using the code in p2_data.py. Please make sure\n",
    "#\t\tyou read the code in that file and understand how it works.\n",
    "\n",
    "df_train = data.df_train\n",
    "df_test = data.df_test\n",
    "\n",
    "X_train = data.X_train\n",
    "y_train = data.y_train\n",
    "X_test = data.X_test\n",
    "y_test = data.y_test\n",
    "\n",
    "# stacking an array of ones\n",
    "X_train = np.hstack((np.ones_like(y_train), X_train))\n",
    "X_test = np.hstack((np.ones_like(y_test), X_test))\n",
    "\n",
    "# one hot encoder\n",
    "enc = OneHotEncoder()\n",
    "y_train_OH = enc.fit_transform(y_train.copy()).astype(int).toarray()\n",
    "y_test_OH = enc.fit_transform(y_test.copy()).astype(int).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(60000, 10)\n",
      "-138155.1055796426\n"
     ]
    }
   ],
   "source": [
    "# get the shape of the data, and initialize nll_list\n",
    "m, n = X_train.shape\n",
    "k = y_train_OH.shape[1]\n",
    "\n",
    "# initialize the weight and its gradient\n",
    "W = np.zeros((n, k))\n",
    "\n",
    "mu = get_mu(X_train, W + .000001)\n",
    "\n",
    "m, n = X_train.shape\n",
    "k = y_train_OH.shape[1]\n",
    "nll_list = []\n",
    "\n",
    "# initialize the weight and its gradient\n",
    "W = np.zeros((n, k)) + .000000001\n",
    "\n",
    "#print(NLL(X_train, y_train_OH, W, .0001))\n",
    "print(y_train_OH.shape)\n",
    "print(mu.shape)\n",
    "\n",
    "test = (y_train_OH * np.log(mu)).sum()\n",
    "\n",
    "print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Running gradient descent...\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "-- Iteration 100 - negative log likelihood  22804.8176\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "-- Iteration 200 - negative log likelihood  20265.7595\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n",
      "(785, 10)\n"
     ]
    }
   ],
   "source": [
    "W, nll_list = grad_descent(X_train, y_train_OH, 0.00001,\\\n",
    "                 eps=1e-6, max_iter=500, print_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Accuracy vs. Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> Step 1: Finding optimal regularization parameter...\n",
      "\n",
      "==> Running gradient descent...\n"
     ]
    }
   ],
   "source": [
    "# =============STEP 1: Accuracy versus lambda=================\n",
    "# NOTE: Fill in the code in NLL, grad_softmax, predict and grad_descent.\n",
    "# \t\tThen, fill in predict and accuracy_vs_lambda\n",
    "\n",
    "print('\\n\\n==> Step 1: Finding optimal regularization parameter...')\n",
    "\n",
    "lambda_list = [0.01, 0.1, 0.5, 1.0, 10.0, 50.0, 100.0, 200.0, 500.0, 1000.0]\n",
    "reg_opt = accuracy_vs_lambda(X_train, y_train_OH, X_test, y_test, lambda_list)\n",
    "print('\\n-- Optimal regularization parameter is {:2.2f}'.format(reg_opt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Convergence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============STEP 2: Convergence plot=================\n",
    "W_gd, nll_list_gd = grad_descent(X_train, y_train_OH, reg=reg_opt, max_iter=1500, lr=2e-5, \\\n",
    "    print_freq=100)\n",
    "print('==> Step 2: Plotting convergence plot...')\n",
    "plt.style.use('ggplot')\n",
    "# Plot the learning curve of NLL vs Iteration\n",
    "# YOUR CODE BELOW\n",
    "nll_gd_plot, = plt.plot(range(len(nll_list_gd)), nll_list_gd)\n",
    "plt.setp(nll_gd_plot, color = 'red')\n",
    "plt.title('Convergence Plot on Softmax Regression with $\\lambda = {:2.2f}$'.format(reg_opt))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('NLL')\n",
    "plt.savefig('hw4pr2b_convergence.png', format = 'png')\n",
    "plt.close()\n",
    "print('==> Plotting completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start file for hw4pr2 part (b) of Big Data Summer 2017\n",
    "\n",
    "The file is seperated into two parts:\n",
    "\t1) the helper functions\n",
    "\t2) the main driver.\n",
    "\n",
    "The helper functions are all functions necessary to finish the problem.\n",
    "The main driver will use the helper functions you finished to report and print\n",
    "out the results you need for the problem.\n",
    "\n",
    "First, please COMMENT OUT any steps other than step 0 in main driver before\n",
    "you finish the corresponding functions for that step. Otherwise, you won't be\n",
    "able to run the program because of errors.\n",
    "\n",
    "After finishing the helper functions for each step, you can uncomment\n",
    "the code in main driver to check the result.\n",
    "\n",
    "Note:\n",
    "1. Please read the instructions and hints carefully, and use the name of the\n",
    "variables we provided, otherwise, the function may not work.\n",
    "\n",
    "2. Remember to comment out the TODO comment after you finish each part.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#########################################\n",
    "#\t\t\t Helper Functions\t    \t#\n",
    "#########################################\n",
    "\n",
    "\n",
    "def get_mu(X, W):\n",
    "    \"\"\"\n",
    "    helper function to find mu\n",
    "    \"\"\"\n",
    "    mu = np.exp(X@W)\n",
    "    mu = np.apply_along_axis(lambda row: row/row.sum(), 1, mu)\n",
    "    return mu\n",
    "\n",
    "\n",
    "\n",
    "def NLL(X, y, W, reg=0.0):\n",
    "    \"\"\"\tThis function takes in four arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) y, the label of the data with OHENC dimension m x c\n",
    "            3) W, a weight matrix\n",
    "            4) reg, the parameter for regularization\n",
    "\n",
    "        This function calculates and returns negative log likelihood for\n",
    "        softmax regression.\n",
    "\n",
    "        HINT:\n",
    "            1) Recall the negative log likelihood function for softmax\n",
    "               regression and\n",
    "            2) Use a.sum() to find the summation of all entries in a numpy\n",
    "               array a\n",
    "            3) When perform operations vertically across rows, we use axis=0.\n",
    "               When perform operations horizontally across columns, we use\n",
    "               axis=1.\n",
    "            4) Use np.exp and np.log to calculate the exp and log of\n",
    "               each entry of the input array\n",
    "\n",
    "        NOTE: Please use the variable given, NLL.\n",
    "    \"\"\"\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    \n",
    "    #Make mu\n",
    "    mu = get_mu(X,W)\n",
    "    \n",
    "    #Don't regularize the bias term\n",
    "    W_no_bias = np.copy(W)\n",
    "    W_no_bias[:,0] = 0\n",
    "    \n",
    "    reg_term = np.apply_along_axis(lambda col: np.norm(col)**2, 0, W_no_bias).sum()\n",
    "\n",
    "    #Now get the NLL\n",
    "    NLL = -(y * np.log(mu)).sum() + reg*reg_term\n",
    "\n",
    "    #NLL = -(y * np.log(mu)).sum() + reg*np.trace(W_no_bias.T @ W_no_bias)\n",
    "\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return NLL\n",
    "\n",
    "\n",
    "\n",
    "def grad_softmax(X, y, W, reg=0.0):\n",
    "    \"\"\"\tThis function takes in four arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) y, the label of the data with OHENC dimension m x c\n",
    "            3) W, a weight matrix\n",
    "            4) reg, the parameter for regularization\n",
    "\n",
    "        This function calculates and returns the gradient of W for softmax\n",
    "        regression.\n",
    "\n",
    "        HINT:\n",
    "            1) Recall the log likelihood function for softmax regression and\n",
    "               get the gradient with respect to the weight matrix, W\n",
    "            2) Remember to apply the regularization\n",
    "\n",
    "        NOTE: Please use the variable given for the gradient, grad.\n",
    "    \"\"\"\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    mu = get_mu(X,W)\n",
    "    \n",
    "    #Don't regularize the bias term\n",
    "    W_no_bias = np.copy(W)\n",
    "    W_no_bias[:,0] = 0\n",
    "    \n",
    "    grad = X.T @ (mu - y) + reg*W_no_bias\n",
    "    \n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "def predict(X, W):\n",
    "    \"\"\"\tThis function takes in two arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) W, a weight matrix\n",
    "\n",
    "        This function returns the predicted labels y_pred with\n",
    "        dimension m x 1\n",
    "\n",
    "        HINT:\n",
    "            1) Firstly obtain the probablity matrix according to the softmax\n",
    "               equation\n",
    "            2) Use np.argmax to get the predicted label for each image\n",
    "\n",
    "        NOTE: Please use the variable given, y_pred.\n",
    "    \"\"\"\n",
    "\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    mu = get_mu(X,W)\n",
    "    \n",
    "    #Get the argmax of each row of mu\n",
    "    y_pred = np.argmax(mu, axis = 1)\n",
    "\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "def get_accuracy(y_pred, y):\n",
    "    \"\"\"\tThis function takes in two arguments:\n",
    "            1) y_pred, the predicted label of data with dimension m x 1\n",
    "            2) y, the true label of data with dimension m x 1\n",
    "\n",
    "        This function calculates and returns the accuracy of the prediction\n",
    "\n",
    "        NOTE: You DO NOT need to change this function\n",
    "    \"\"\"\n",
    "    diff = (y_pred == y).astype(int)\n",
    "    accu = 1. * diff.sum() / len(y)\n",
    "    return accu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X, y, reg=0.0, lr=1e-5, eps=1e-6, max_iter=500, print_freq=20):\n",
    "    \"\"\"\tThis function takes in seven arguments:\n",
    "            1) X, the data with dimension m x (n + 1)\n",
    "            2) y, the label of data with OHENC dimension m x c\n",
    "            3) reg, the parameter for regularization\n",
    "            4) lr, the learning rate\n",
    "            5) eps, the threshold of the norm for the gradients\n",
    "            6) max_iter, the maximum number of iterations\n",
    "            7) print_freq, the frequency of printing the report\n",
    "\n",
    "        This function returns W, the optimal weight by gradient descent,\n",
    "        and nll_list, the corresponding learning objectives.\n",
    "    \"\"\"\n",
    "    # get the shape of the data, and initialize nll_list\n",
    "    m, n = X.shape\n",
    "    k = y.shape[1]\n",
    "    nll_list = []\n",
    "\n",
    "    # initialize the weight and its gradient\n",
    "    W = np.zeros((n, k))\n",
    "    W_grad = np.ones((n, k))\n",
    "\n",
    "\n",
    "    print('\\n==> Running gradient descent...')\n",
    "\n",
    "    # Start iteration for gradient descent\n",
    "    iter_num = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    # HINT: Run the gradient descent algorithm followed steps below\n",
    "    #\t1) Calculate the negative log likelihood at each iteration use function\n",
    "    #\t   NLL defined above\n",
    "    #\t2) Use np.isnan to test element-wise for NaN in obtained nll. If isnan,\n",
    "    #\t   break out of the while loop\n",
    "    #\t3) Otherwise, append the nll to the nll_list\n",
    "    #\t4) Calculate the gradient for W using grad_softmax defined above\n",
    "    #\t5) Upgrade W\n",
    "    #\t6) Keep iterating while the number of iterations is less than the\n",
    "    #\t   maximum and the gradient is larger than the threshold\n",
    "\n",
    "    # NOTE: When calculating negative log likelihood at each iteration, please\n",
    "    #\t\tuse variable name nll to store the value. Otherwise, there might be\n",
    "    # \t\terror when you run the code.\n",
    "\n",
    "    while iter_num < max_iter and np.linalg.norm(W_grad) > eps:\n",
    "\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        \n",
    "        #Calculate and append nll\n",
    "        nll = NLL(X,y,W,reg)\n",
    "        nll_list.append(nll)\n",
    "        \n",
    "        #Calculate the gradient\n",
    "        W_grad = grad_softmax(X,y,W,reg)\n",
    "        \n",
    "        #Upgrade W by moving to minimize W (i.e. moving in the negative gradient direction)\n",
    "        W -= lr*W_grad\n",
    "\n",
    "        \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "        # Print statements for debugging\n",
    "        if (iter_num + 1) % print_freq == 0:\n",
    "            print('-- Iteration {} - negative log likelihood {: 4.4f}'.format(\\\n",
    "                    iter_num + 1, nll))\n",
    "\n",
    "        # Goes to the next iteration\n",
    "        iter_num += 1\n",
    "\n",
    "\n",
    "    # benchmark\n",
    "    t_end = time.time()\n",
    "    print('-- Time elapsed for running gradient descent: {t:2.2f} seconds'.format(\\\n",
    "            t=t_end - t_start))\n",
    "\n",
    "    return W, nll_list\n",
    "\n",
    "\n",
    "\n",
    "def accuracy_vs_lambda(X_train, y_train_OH, X_test, y_test, lambda_list):\n",
    "    \"\"\"\tThis function takes in five arguments:\n",
    "            1) X_train, the training data with dimension m x (n + 1)\n",
    "            2) y_train_OH, the label of training data with dimension m x c\n",
    "            3) X_test, the validation data with dimension m x (n + 1)\n",
    "            4) y_test, the label of validation data with dimension m x 1\n",
    "            5) lambda_list, a list of different regularization paramters that\n",
    "                            we want to test\n",
    "\n",
    "        This function generates a plot of accuracy of prediction vs lambda and\n",
    "        returns the regularization parameter that maximizes the accuracy,\n",
    "        reg_opt.\n",
    "\n",
    "        HINT: generate the list of accuracy following the steps below:\n",
    "            1) Run gradient descent with each parameter to obtain the optimal\n",
    "               weight\n",
    "            2) Predicted the label using the weights\n",
    "            3) Use get_accuracy function provided to calculate the accuracy\n",
    "    \"\"\"\n",
    "    # initialize the list of accuracy\n",
    "    accu_list = []\n",
    "\n",
    "    for reg in lambda_list:\n",
    "\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        W, nll_list = grad_descent(X_train, y_train_OH, reg,\\\n",
    "                 eps=1e-6, max_iter=500, print_freq=100)\n",
    "            \n",
    "        predictions = predict(X_test,W)\n",
    "        accuracy = get_accuracy(predictions, y)\n",
    "        \n",
    "        accu_list.append(accuracy)\n",
    "        \n",
    "\n",
    "        \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "        print('-- Accuracy is {:2.4f} for lambda = {:2.2f}'.format(accuracy, reg))\n",
    "\n",
    "\n",
    "    # Plot accuracy vs lambda\n",
    "    print('==> Printing accuracy vs lambda...')\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(lambda_list, accu_list)\n",
    "    plt.title('Accuracy versus Lambda in Softmax Regression')\n",
    "    plt.xlabel('Lambda')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.savefig('hw4pr2b_lva.png', format = 'png')\n",
    "    plt.close()\n",
    "    print('==> Plotting completed.')\n",
    "\n",
    "\n",
    "    # NOTE: use the variable given, reg_opt\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    \n",
    "    maxdex = argmax(accu_list)\n",
    "    reg_opt = lambda_list[maxdex]\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "    return reg_opt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
