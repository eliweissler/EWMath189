{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> Step 1: Running logistic regression...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# =============STEP 0: LOADING DATA=================\n",
    "# NOTE: The data is loaded using the code in p2_data.py. Please make sure\n",
    "#\t\tyou read the code in that file and understand how it works.\n",
    "\n",
    "# data frame\n",
    "df_train = data.df_train\n",
    "df_test = data.df_test\n",
    "\n",
    "# training data\n",
    "X_train = data.X_train\n",
    "y_train = data.y_train\n",
    "\n",
    "# test data\n",
    "X_test = data.X_test\n",
    "y_test = data.y_test\n",
    "\n",
    "# =============STEP 1: Logistic regression=================\n",
    "print('\\n\\n==> Step 1: Running logistic regression...')\n",
    "\n",
    "# splitting data for logistic regression\n",
    "# NOTE: for logistic regression, we only want images with label 0 or 1.\n",
    "df_train_logreg = df_train[df_train.label <= 1]\n",
    "df_test_logreg = df_test[df_test.label <= 1]\n",
    "\n",
    "# training data for logistic regression\n",
    "X_train_logreg = np.array(df_train_logreg[:][[col for \\\n",
    "    col in df_train_logreg.columns if col != 'label']]) / 256.\n",
    "y_train_logreg = np.array(df_train_logreg[:][['label']])\n",
    "\n",
    "# testing data for logistic regression\n",
    "X_test_logreg = np.array(df_test_logreg[:][[col for \\\n",
    "    col in df_test_logreg.columns if col != 'label']]) / 256.\n",
    "y_test_logreg = np.array(df_test_logreg[:][['label']])\n",
    "\n",
    "# stacking a column of 1's to both training and testing data\n",
    "X_train_logreg = np.hstack((np.ones_like(y_train_logreg), X_train_logreg))\n",
    "X_test_logreg = np.hstack((np.ones_like(y_test_logreg), X_test_logreg))\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Grad Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Step 1a: Running gradient descent...\n",
      "\n",
      "==> Running gradient descent...\n",
      "-- Iteration 100 -                     negative log likelihood  74.7108\n",
      "-- Iteration 200 -                     negative log likelihood  56.7824\n",
      "-- Iteration 300 -                     negative log likelihood  48.7248\n",
      "-- Iteration 400 -                     negative log likelihood  43.6566\n",
      "-- Iteration 500 -                     negative log likelihood  39.9624\n",
      "-- Time elapsed for running gradient descent: 32.95 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========STEP 1a: Gradient descent=========\n",
    "# NOTE: Fill in the code in grad_logreg, NLL and grad_descent for this step\n",
    "\n",
    "print('\\n==> Step 1a: Running gradient descent...')\n",
    "W_gd, nll_list_gd = grad_descent(X_train_logreg, y_train_logreg, reg = 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Step 1b: Running Newton's method...\n",
      "==> Running Newton's method...\n",
      "step size is 2.0\n",
      "-- Iteration 1 - negative log likelihood  8778.9096\n",
      "step size is 2.0\n",
      "-- Iteration 2 - negative log likelihood  399.9191\n",
      "step size is 2.0\n",
      "-- Iteration 3 - negative log likelihood  76.2661\n",
      "step size is 2.0\n",
      "-- Iteration 4 - negative log likelihood  14.0647\n",
      "step size is 2.0\n",
      "-- Iteration 5 - negative log likelihood  2.4256\n",
      "step size is 2.0\n",
      "-- Iteration 6 - negative log likelihood  0.3962\n",
      "step size is 0.8\n",
      "-- Iteration 7 - negative log likelihood  0.0742\n",
      "step size is 2.0\n",
      "-- Iteration 8 - negative log likelihood  0.0336\n",
      "step size is 1.6\n",
      "-- Iteration 9 - negative log likelihood  0.0103\n",
      "step size is 1.6\n",
      "-- Iteration 10 - negative log likelihood  0.0050\n",
      "step size is 1.4000000000000001\n",
      "-- Iteration 11 - negative log likelihood  0.0030\n",
      "step size is 1.4000000000000001\n",
      "-- Iteration 12 - negative log likelihood  0.0022\n",
      "step size is 1.2000000000000002\n",
      "-- Iteration 13 - negative log likelihood  0.0018\n",
      "step size is 1.0\n",
      "-- Iteration 14 - negative log likelihood  0.0016\n",
      "step size is 0.4\n",
      "-- Iteration 15 - negative log likelihood  0.0016\n",
      "step size is 0.0\n",
      "-- Iteration 16 - negative log likelihood  0.0016\n",
      "-- Time elapsed for running Newton's method: 76.00 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========STEP 1b: Newton's method==========\n",
    "# NOTE: Fill in the code in newton_step and newton_method for this step\n",
    "\n",
    "print('\\n==> Step 1b: Running Newton\\'s method...')\n",
    "W_newton, nll_list_newton = newton_method(X_train_logreg, y_train_logreg, \\\n",
    "    reg = 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Step 2: Generate Convergence Plot...\n",
      "==> Plotting convergence plot...\n",
      "==> Plotting completed.\n",
      "\n",
      "Step 3: ==> Generating plots for accuracy, precision, recall, and F-1 score...\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  2.4256\n",
      "-- Iteration 10 - negative log likelihood  0.0050\n",
      "-- Iteration 15 - negative log likelihood  0.0016\n",
      "-- Time elapsed for running Newton's method: 76.70 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  2.7734\n",
      "-- Iteration 10 - negative log likelihood  0.0696\n",
      "-- Time elapsed for running Newton's method: 66.45 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  7.0967\n",
      "-- Iteration 10 - negative log likelihood  1.9084\n",
      "-- Time elapsed for running Newton's method: 45.61 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  14.6890\n",
      "-- Time elapsed for running Newton's method: 49.62 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  25.5050\n",
      "-- Time elapsed for running Newton's method: 32.90 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  34.4012\n",
      "-- Time elapsed for running Newton's method: 33.01 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  38.1484\n",
      "-- Time elapsed for running Newton's method: 32.31 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  53.5325\n",
      "-- Time elapsed for running Newton's method: 27.96 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  75.9217\n",
      "-- Time elapsed for running Newton's method: 27.17 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  93.7841\n",
      "-- Time elapsed for running Newton's method: 26.74 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  123.1704\n",
      "-- Time elapsed for running Newton's method: 28.71 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  153.1917\n",
      "-- Time elapsed for running Newton's method: 22.96 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  179.9704\n",
      "-- Time elapsed for running Newton's method: 22.48 seconds\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - negative log likelihood  224.8378\n",
      "-- Time elapsed for running Newton's method: 22.27 seconds\n",
      "==> Plotting completed.\n",
      "\n",
      "==> Optimal regularization parameter is 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============STEP 2: Generate convergence plot=================\n",
    "# NOTE: You DO NOT need to fill in any additional helper function for this\n",
    "# \t\tstep to run. This step uses what you implemented for the previous\n",
    "#\t\ttwo steps to plot.\n",
    "print('\\n==> Step 2: Generate Convergence Plot...')\n",
    "print('==> Plotting convergence plot...')\n",
    "\n",
    "# set up the style for the plot\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# plot gradient descent and newton's method convergence plot\n",
    "nll_gd_plot, = plt.plot(range(len(nll_list_gd)), nll_list_gd)\n",
    "plt.setp(nll_gd_plot, color = 'red')\n",
    "\n",
    "nll_newton_plot, = plt.plot(range(len(nll_list_newton)), nll_list_newton)\n",
    "plt.setp(nll_newton_plot, color = 'green')\n",
    "\n",
    "# add legend, titles, etc. for the plots\n",
    "plt.legend((nll_gd_plot, nll_newton_plot), \\\n",
    "    ('Gradient descent', 'Newton\\'s method'), loc = 'best')\n",
    "plt.title('Convergence Plot on Binary MNIST Classification')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('NLL')\n",
    "plt.savefig('hw4pr2a_convergence.png', format = 'png')\n",
    "plt.close()\n",
    "\n",
    "print('==> Plotting completed.')\n",
    "\n",
    "# =============STEP 3: Generate accuracy/precision plot=================\n",
    "# NOTE: Fill in the code in get_description and plot_description for this Step\n",
    "\n",
    "print('\\nStep 3: ==> Generating plots for accuracy, precision, recall, and F-1 score...')\n",
    "\n",
    "# Plot the graph and obtain the optimal regularization parameter\n",
    "reg_opt = plot_description(X_train_logreg, y_train_logreg, \\\n",
    "    X_test_logreg, y_test_logreg)\n",
    "\n",
    "print('\\n==> Optimal regularization parameter is {:4.4f}'.format(reg_opt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start file for hw4pr2 part(a) of Big Data Summer 2017\n",
    "\n",
    "The file is seperated into two parts:\n",
    "\t1) the helper functions\n",
    "\t2) the main driver.\n",
    "\n",
    "The helper functions are all functions necessary to finish the problem.\n",
    "The main driver will use the helper functions you finished to report and print\n",
    "out the results you need for the problem.\n",
    "\n",
    "First, please COMMENT OUT any steps other than step 0 in main driver before\n",
    "you finish the corresponding functions for that step. Otherwise, you won't be\n",
    "able to run the program because of errors.\n",
    "\n",
    "After finishing the helper functions for each step, you can uncomment\n",
    "the code in main driver to check the result.\n",
    "\n",
    "Note:\n",
    "1. Please read the instructions and hints carefully, and use the name of the\n",
    "variables we provided, otherwise, the function may not work.\n",
    "\n",
    "2. Remember to comment out the TODO comment after you finish each part.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Helper Functions#\n",
    "#########################################\n",
    "\n",
    "import p2_data as data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#########################\n",
    "#\t    Step 1a`\t\t#\n",
    "#########################\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\tThis function takes in one argument:\n",
    "            1) x, a numpy array\n",
    "\n",
    "        This function applies the sigmoid / logistic function on each entry of\n",
    "        the input array returns the new array.\n",
    "\n",
    "        NOTE: You don't need to change this function.\n",
    "    \"\"\"\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "def grad_logreg(X, y, W, reg=0.0):\n",
    "    \"\"\"\tThis function takes in four arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) y, the label of the data with dimension m x 1\n",
    "            3) W, a weight matrix with bias\n",
    "            4) reg, the parameter for regularization\n",
    "\n",
    "        This function calculates and returns the gradient of W for logistic\n",
    "        regression.\n",
    "\n",
    "        HINT:\n",
    "            1) Recall the log likelihood function for logistic regression and\n",
    "               get the gradient with respect to the weight matrix, W\n",
    "            2) Remember to apply the l2 regularization\n",
    "            3) You will need to use the sigmoid function above\n",
    "\n",
    "        NOTE: Please use the variable given for the gradient, grad.\n",
    "    \"\"\"\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    \n",
    "    #From textbook page 252\n",
    "    W_no_bias = np.copy(W) #Make a deep copy\n",
    "    W_no_bias[0] = 0 #Don't regularize the bias term\n",
    "    grad = X.T @ (sigmoid(X@W) - y) + reg*W_no_bias\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NLL(X, y, W, reg=0.0):\n",
    "    \"\"\"\tThis function takes in four arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) y, the label of the data with dimension m x 1\n",
    "            3) W, a weight matrix with bias\n",
    "            4) reg, the parameter for regularization\n",
    "\n",
    "        This function calculates and returns negative log likelihood of the\n",
    "        logistic regression with l2 regularization\n",
    "\n",
    "        HINT:\n",
    "            1) Recall the negative log likelihood function for logistic regression.\n",
    "            2) Use a.sum() to find the summation of all entries in a numpy\n",
    "               array a\n",
    "            3) Use np.linalg.norm to find the norm of a given vector\n",
    "            4) Use np.log to caculate the log of each entry of the input array\n",
    "\n",
    "        NOTE: please use the variable given for the final returned result, nll.\n",
    "    \"\"\"\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    #From textbook page 252 and hw2pr1\n",
    "    mu = sigmoid(X@W)\n",
    "    #Get the NLL. When mu is basically 1 this throws errors, so prevent it from being 1\n",
    "    mu = np.apply_along_axis(lambda m: 0.9999999 if m == 1. else m,1,mu)\n",
    "    nll = -(y.T @ np.log(mu) + (1 - y.T) @ np.log(1 - mu))\n",
    "    \n",
    "    #Add in the regularization term -- don't regularize the bias term\n",
    "    W_no_bias = np.copy(W)\n",
    "    W_no_bias[0] = 0\n",
    "    nll += reg*np.linalg.norm(W_no_bias)**2\n",
    "    \n",
    "    \n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return nll[0,0]\n",
    "\n",
    "\n",
    "\n",
    "def grad_descent(X, y, reg=0.0, lr=1e-4, eps=1e-6, max_iter=500, print_freq=100):\n",
    "    \"\"\"\tThis function takes in seven arguments:\n",
    "            1) X, the data with dimension m x (n + 1)\n",
    "            2) y, the label of data with dimension m x 1\n",
    "            3) reg, the parameter for regularization\n",
    "            4) lr, the learning rate\n",
    "            5) eps, the threshold of the norm for the gradients\n",
    "            6) max_iter, the maximum number of iterations\n",
    "            7) print_freq, the frequency of printing the report\n",
    "\n",
    "        This function returns W, the optimal weight by gradient descent,\n",
    "        and nll_list, the corresponding learning objectives.\n",
    "    \"\"\"\n",
    "    # get the shape of the data, and initiate nll list\n",
    "    m, n = X.shape\n",
    "    nll_list = []\n",
    "\n",
    "    # initialize the weight and its gradient\n",
    "    W = np.zeros((n, 1))\n",
    "    W_grad = np.ones_like(W)\n",
    "\n",
    "\n",
    "    print('\\n==> Running gradient descent...')\n",
    "\n",
    "    # Start iteration for gradient descent\n",
    "    iter_num = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "\n",
    "    # TODO: run gradient descent algorithms\n",
    "\n",
    "    # HINT: Run the gradient descent algorithm followed steps below\n",
    "    #1) Calculate the negative log likelihood at each iteration and\n",
    "    #   append the value to nll_list\n",
    "    #2) Calculate the gradient for W\n",
    "    #3) Upgrade W\n",
    "    #4) Keep iterating while the number of iterations is less than the\n",
    "    #   maximum and the gradient is larger than the threshold\n",
    "\n",
    "    # NOTE: When calculating negative log likelihood at each iteration, please\n",
    "    #       use variable name nll to store the value. Otherwise, there might be\n",
    "    #       error when you run the code.\n",
    "\n",
    "    while iter_num < max_iter and np.linalg.norm(W_grad) > eps:\n",
    "\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        #Calculate and append nll\n",
    "        nll = NLL(X,y,W,reg)\n",
    "        nll_list.append(nll)\n",
    "        \n",
    "        #Calculate the gradient\n",
    "        W_grad = grad_logreg(X,y,W,reg)\n",
    "        \n",
    "        #Upgrade W by moving to minimize W (i.e. moving in the negative gradient direction)\n",
    "        W -= lr*W_grad\n",
    "        \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "        # Print statements for debugging\n",
    "        if (iter_num + 1) % print_freq == 0:\n",
    "            print('-- Iteration {0} - \\\n",
    "                    negative log likelihood {1: 4.4f}'.format(iter_num + 1, nll))\n",
    "\n",
    "        # Goes to the next iteration\n",
    "        iter_num += 1\n",
    "\n",
    "\n",
    "    # benchmark\n",
    "    t_end = time.time()\n",
    "    print('-- Time elapsed for running gradient descent: {t:2.2f} seconds'.format(\\\n",
    "            t = t_end - t_start))\n",
    "\n",
    "    return W, nll_list\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "#\t    Step 1a`\t\t#\n",
    "#########################\n",
    "\n",
    "def newton_step(X, y, W, reg=0.0):\n",
    "    \"\"\"\tThis function takes in four arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) y, the label of the data with dimension m x 1\n",
    "            3) W, a weight matrix with bias\n",
    "            4) reg, the parameter for regularization\n",
    "\n",
    "        This function calculates and returns the change of W according to\n",
    "        the Newton's method\n",
    "\n",
    "        HINT: get the result following the steps below\n",
    "            1) Calculate the gradient of log likelihood, grad, with respect to W\n",
    "            2) Use np.diag to create a diagonal matrix, with mu*(1-mu) being the\n",
    "               entries on the diagonal\n",
    "            3) Calculate the Hessian matrix, H, of logistic regression following\n",
    "               the equation (you will need to use the diagonal matrix created)\n",
    "            4) Using np.linalg.solve to solve for d in the equation Hd = -grad\n",
    "\n",
    "        NOTE: Please use the variable given for final returned result, d.\n",
    "    \"\"\"\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "        \n",
    "    #Get gradient and Hessian\n",
    "    grad = grad_logreg(X,y,W,reg)\n",
    "    \n",
    "    #Hessian from hw2pr1 and textbook page 252\n",
    "    mu = sigmoid(X @ W)\n",
    "    S = np.diag(mu[:,0]*(1-mu[:,0]))\n",
    "    #Dont regularize the bias term\n",
    "    reg_term = reg*np.eye(X.shape[1])\n",
    "    reg_term[0] = 0\n",
    "    H = X.T @ S @ X + reg_term\n",
    "    \n",
    "    #Get direction to move in\n",
    "    d = np.linalg.solve(H,-grad)\n",
    "\n",
    "    #Do a rudimentory line search to figure out how far to move\n",
    "    step_sizes = np.linspace(0,2,11)\n",
    "    likelihoods = [NLL(X,y,W+s*d,reg) for s in step_sizes]\n",
    "    mindex = np.argmin(likelihoods)\n",
    "    step_size = step_sizes[mindex]\n",
    "\n",
    "    d = step_size*d\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def newton_method(X, y, reg=0.0, eps=1e-6, max_iter=20, print_freq=1):\n",
    "    \"\"\"\tThis function takes in six arguments:\n",
    "            1) X, the data with dimension m x (n + 1)\n",
    "            2) y, the label of data with dimension m x 1\n",
    "            3) reg, the parameter for regularization\n",
    "            4) eps, the threshold of the norm for the gradients\n",
    "            5) max_iter, the maximum number of iterations\n",
    "            6) print_freq, the frequency of printing the report\n",
    "\n",
    "        This function returns W, the optimal weight by Newton's Method,\n",
    "        and nll_list, the corresponding learning objectives.\n",
    "    \"\"\"\n",
    "    # get the shape of the data, and initiate nll list\n",
    "    m, n = X.shape\n",
    "    nll_list = []\n",
    "\n",
    "    # initialize the weight and its gradient\n",
    "    W = np.zeros((n, 1)) + eps #Avoid singular matrix error\n",
    "    step = np.ones_like(W)\n",
    "\n",
    "    print('==> Running Newton\\'s method...')\n",
    "\n",
    "    # Start iteration for gradient descent\n",
    "    iter_num = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    # HINT: Run the gradient descent algorithm followed steps below\n",
    "    #\t1) Calculate the negative log likelihood at each iteration and\n",
    "    #\t   append the value to nll_list\n",
    "    #\t2) Calculate the gradient for W using newton_step defined above\n",
    "    #\t3) Upgrade W\n",
    "    #\t4) Keep iterating while the number of iterations is less than the\n",
    "    #\t   maximum and the gradient is larger than the threshold\n",
    "\n",
    "    while iter_num < max_iter and np.linalg.norm(step) > eps:\n",
    "\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        #Calculate and append nll\n",
    "        nll = NLL(X,y,W,reg)\n",
    "        nll_list.append(nll)\n",
    "        \n",
    "        #Calculate the newton step\n",
    "        step = newton_step(X,y,W,reg)\n",
    "        \n",
    "        #Upgrade W \n",
    "        W += step\n",
    "\n",
    "\n",
    "        \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "                \n",
    "        # Print statements for debugging\n",
    "        if (iter_num + 1) % print_freq == 0:\n",
    "            print('-- Iteration {} - negative log likelihood {: 4.4f}'.format(\\\n",
    "                    iter_num + 1, nll))\n",
    "\n",
    "        # Goes to the next iteration\n",
    "        iter_num += 1\n",
    "\n",
    "\n",
    "\n",
    "    # benchmark\n",
    "    t_end = time.time()\n",
    "    print('-- Time elapsed for running Newton\\'s method: {t:2.2f} seconds'.format(\\\n",
    "            t = t_end - t_start))\n",
    "\n",
    "    return W, nll_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "#\t\t Step 3\t\t\t#\n",
    "#########################\n",
    "\n",
    "def predict(X, W):\n",
    "    \"\"\"\tThis function takes in two arguments:\n",
    "            1) W, a weight matrix with bias\n",
    "            2) X, the data with dimension m x (n + 1)\n",
    "\n",
    "        This function calculates and returns the predicted label.\n",
    "\n",
    "        NOTE: You don't need to change this function.\n",
    "    \"\"\"\n",
    "    mu = sigmoid(X @ W)\n",
    "    return (mu >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "def get_description(X, y, W):\n",
    "    \"\"\"\tThis function takes in three arguments:\n",
    "            1) X, the data matrix with dimension m x (n + 1)\n",
    "            2) y, the label of the data with dimension m x 1.\n",
    "            3) W, the weight matrix with bias and dimension (n + 1) x 1\n",
    "\n",
    "        This function calculates and returns the accuracy, precision,\n",
    "        recall and F-1 score of the prediction.\n",
    "\n",
    "        HINT:\n",
    "            1) Get the predict lables using predict defined above\n",
    "            2) Accuracy = probability of correct prediction\n",
    "            3) Precision = probability of true label being 1 given that the\n",
    "               predicted label is 1\n",
    "            4) Recall = probablity of predicted label being 1 given that the\n",
    "               true label is 1\n",
    "            5) F-1 = 2*p*r / (p + r), where p = precision and r = recall\n",
    "\n",
    "        NOTE: Please use the variable given for final returned results.\n",
    "    \"\"\"\n",
    "    # TODO: Find the accuracy, precision, recall, and f-1 score of the prediction\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    prediction = predict(X, W)\n",
    "    correct = 0\n",
    "    P_correct = 0\n",
    "    P_incorrect = 0\n",
    "    R_correct = 0\n",
    "    R_incorrect = 0\n",
    "    total = X.shape[0]\n",
    "    for i in range(0,total):\n",
    "        if prediction[i] == y[i]:\n",
    "            correct+=1\n",
    "        if prediction[i] == 1:\n",
    "            if y[i] == 1:\n",
    "                P_correct+=1\n",
    "            else:\n",
    "                P_incorrect+=1\n",
    "        if y[i] == 1:\n",
    "            if prediction[i] == 1:\n",
    "                R_correct+=1\n",
    "            else:\n",
    "                R_incorrect+=1\n",
    "    accuracy = correct/total\n",
    "    precision = P_correct/(P_correct+P_incorrect)\n",
    "    recall = R_correct/(R_correct+R_incorrect)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    \n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_description(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\tThis function takes in four arguments:\n",
    "            1) X_train, the training data with dimension m x (n + 1)\n",
    "            2) y_train, the label of training data with dimension m x 1\n",
    "            3) X_val, the validation data with dimension m x (n + 1)\n",
    "            4) y_val, the label of validation data with dimension m x 1\n",
    "\n",
    "        This function plots the accuracy/precision/recall/F-1 score versus\n",
    "        lambda and returns the lambda that maximizes accuracy.\n",
    "    \"\"\"\n",
    "    reg_list = []\n",
    "    a_list = []\n",
    "    p_list = []\n",
    "    r_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    # TODO: Find the list of accuracy, precision, recall, and f-1 score of the\n",
    "    # prediction given a list of different lambda\n",
    "\n",
    "    # HINT:\n",
    "    # \t1) First, generate/create a list of different lambda\n",
    "    #   2) For each lambda, run gradient descent and obtain the optimal weights\n",
    "    # \t3) Get accuracy, precision, recall and f1 with the data and W_opt\n",
    "    #\t   obtained, and append those into the corresponding list\n",
    "\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    reg_list = [10**-6, 10**-4, 10**-2, .1, .4, .8, 1, 2, 4, 6, 10, 15, 20, 30]\n",
    "    reg_list.sort()\n",
    "    \n",
    "    for i in range(0,len(reg_list)):\n",
    "        W, nll_list = newton_method(X_train, y_train, \\\n",
    "            reg = reg_list[i], print_freq = 5)\n",
    "        accuracy, precision, recall, f1 = get_description(X_test,y_test,W)\n",
    "        a_list.append(accuracy)\n",
    "        p_list.append(precision)\n",
    "        r_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "\n",
    "\n",
    "    # Generate plots\n",
    "    # plot accurary versus lambda\n",
    "    a_vs_lambda_plot, = plt.plot(reg_list, a_list)\n",
    "    plt.setp(a_vs_lambda_plot, color = 'red')\n",
    "\n",
    "    # plot precision versus lambda\n",
    "    p_vs_lambda_plot, = plt.plot(reg_list, p_list)\n",
    "    plt.setp(p_vs_lambda_plot, color = 'green')\n",
    "\n",
    "    # plot recall versus lambda\n",
    "    r_vs_lambda_plot, = plt.plot(reg_list, r_list)\n",
    "    plt.setp(r_vs_lambda_plot, color = 'blue')\n",
    "\n",
    "    # plot f1 score versus lambda\n",
    "    f1_vs_lambda_plot, = plt.plot(reg_list, f1_list)\n",
    "    plt.setp(f1_vs_lambda_plot, color = 'yellow')\n",
    "\n",
    "    # Set up the legend, titles, etc. for the plots\n",
    "    plt.legend((a_vs_lambda_plot, p_vs_lambda_plot, r_vs_lambda_plot, \\\n",
    "        f1_vs_lambda_plot), ('accuracy', 'precision', 'recall', 'F-1'),\\\n",
    "         loc = 'best')\n",
    "    plt.title('Testing descriptions')\n",
    "    plt.xlabel('regularization parameter')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.savefig('hw4pr2a_description.png', format = 'png')\n",
    "    plt.close()\n",
    "\n",
    "    print('==> Plotting completed.')\n",
    "\n",
    "\n",
    "    # TODO: Find the lambda, reg_opt, that maximizes accuracy\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    mindex = np.argmax(a_list)\n",
    "    reg_opt = reg_list[mindex]\n",
    "\n",
    "    \"*** END YOUR CODE HERE ***\"\n",
    "    return reg_opt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
